{
  "version": "1.0.0",
  "default_model": "gemini25fl",
  "models": {
    "localModel": {
      "model_id": "Qwen2.5-7B-Instruct-Q4_K_M.gguf",
      "type": "OPENAI",
      "host": "http://localhost:8000/v1",
      "env_key": null,
      "max_context": 50,
      "enabled": false,
      "description": "Local llama.cpp / vllm server â€” set host to your server URL and enable",
      "tool_call_available": true,
      "llm_call_timeout": 120,
      "llm_tools": [
        "core",
        "db"
      ],
      "system_prompt_folder": "system_prompt/000_default",
      "temperature": 0.0,
      "top_p": 0.0,
      "top_k": null,
      "token_selection_setting": "custom"
    },
    "gpt5m": {
      "model_id": "gpt-5-mini",
      "type": "OPENAI",
      "host": "https://api.openai.com/v1",
      "env_key": "OPENAI_API_KEY",
      "max_context": 2000,
      "enabled": true,
      "description": "OpenAI gpt-5-mini",
      "tool_call_available": true,
      "llm_call_timeout": 60,
      "llm_tools": [
        "core",
        "admin",
        "db",
        "search",
        "drive",
        "vscode",
        "extract",
        "agent"
      ],
      "system_prompt_folder": "system_prompt/000_default",
      "temperature": 1.0,
      "top_p": 1.0,
      "top_k": null,
      "token_selection_setting": "default"
    },
    "gpt5n": {
      "model_id": "gpt-5-nano",
      "type": "OPENAI",
      "host": "https://api.openai.com/v1",
      "env_key": "OPENAI_API_KEY",
      "max_context": 2000,
      "enabled": true,
      "description": "OpenAI gpt-5-nano",
      "tool_call_available": true,
      "llm_call_timeout": 60,
      "llm_tools": [
        "core",
        "admin",
        "db",
        "search",
        "drive",
        "vscode",
        "extract",
        "agent"
      ],
      "system_prompt_folder": "system_prompt/000_default",
      "temperature": 1.0,
      "top_p": 1.0,
      "top_k": null,
      "token_selection_setting": "default"
    },
    "gpt4om": {
      "model_id": "gpt-4o-mini",
      "type": "OPENAI",
      "host": "https://api.openai.com/v1",
      "env_key": "OPENAI_API_KEY",
      "max_context": 2000,
      "enabled": true,
      "description": "OpenAI gpt-4o-mini",
      "tool_call_available": true,
      "llm_call_timeout": 60,
      "llm_tools": [
        "core",
        "admin",
        "db",
        "search",
        "drive",
        "vscode",
        "extract",
        "agent"
      ],
      "system_prompt_folder": "system_prompt/000_default",
      "temperature": 1.0,
      "top_p": 1.0,
      "top_k": null,
      "token_selection_setting": "default"
    },
    "grok41fr": {
      "model_id": "grok-4-1-fast-reasoning",
      "type": "OPENAI",
      "host": "https://api.x.ai/v1",
      "env_key": "XAI_API_KEY",
      "max_context": 5000,
      "enabled": true,
      "description": "xAI grok-4-1-fast-reasoning",
      "tool_call_available": true,
      "llm_call_timeout": 60,
      "llm_tools": [
        "core",
        "admin",
        "db",
        "search",
        "drive",
        "vscode",
        "extract",
        "agent"
      ],
      "system_prompt_folder": "system_prompt/000_default",
      "temperature": 1.0,
      "top_p": 1.0,
      "top_k": null,
      "token_selection_setting": "default"
    },
    "grok41fnr": {
      "model_id": "grok-4-1-fast-non-reasoning",
      "type": "OPENAI",
      "host": "https://api.x.ai/v1",
      "env_key": "XAI_API_KEY",
      "max_context": 5000,
      "enabled": true,
      "description": "xAI grok-4-1-fast-non-reasoning",
      "tool_call_available": true,
      "llm_call_timeout": 60,
      "llm_tools": [
        "core",
        "admin",
        "db",
        "search",
        "drive",
        "vscode",
        "extract",
        "agent"
      ],
      "system_prompt_folder": "system_prompt/000_default",
      "temperature": 1.0,
      "top_p": 1.0,
      "top_k": null,
      "token_selection_setting": "default"
    },
    "gemini25f": {
      "model_id": "gemini-2.5-flash",
      "type": "GEMINI",
      "host": null,
      "env_key": "GEMINI_API_KEY",
      "max_context": 10000,
      "enabled": true,
      "description": "Google gemini-2.5-flash",
      "tool_call_available": true,
      "llm_call_timeout": 60,
      "llm_tools": [
        "core",
        "db",
        "search",
        "drive",
        "vscode",
        "extract"
      ],
      "system_prompt_folder": "system_prompt/000_default",
      "temperature": 1.0,
      "top_p": 0.95,
      "top_k": 40,
      "token_selection_setting": "default",
      "llm_tools_gates": ["db_query"]
    },
    "gemini25fl": {
      "model_id": "gemini-2.5-flash-lite",
      "type": "GEMINI",
      "host": null,
      "env_key": "GEMINI_API_KEY",
      "max_context": 10000,
      "enabled": true,
      "description": "Google gemini-2.5-flash-lite",
      "tool_call_available": true,
      "llm_call_timeout": 60,
      "llm_tools": [
        "core",
        "db",
        "search",
        "drive",
        "vscode",
        "extract"
      ],
      "system_prompt_folder": "system_prompt/000_default",
      "temperature": 1.0,
      "top_p": 0.95,
      "top_k": 40,
      "token_selection_setting": "default",
      "llm_tools_gates": []
    }
  },
  "limits": {
    "max_at_llm_depth": 1,
    "max_agent_call_depth": 1,
    "max_tool_iterations": 100
  },
  "metadata": {
    "last_modified": "2026-02-24",
    "notes": "LLM model registry. Modified by agentctl.py model commands. setup-agent-mcp.sh copies your reference install's llm-models.json over this template."
  }
}
