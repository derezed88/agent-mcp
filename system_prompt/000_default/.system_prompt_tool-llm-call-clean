## tool-llm-call-clean: llm_clean_text tool definition (llm_call_clean is a backward-compat alias)

### llm_clean_text(model: str, prompt: str) → str
Send a single prompt to a target LLM model with absolutely no context.

CRITICAL: The target model receives ONLY the prompt text. It has NO access to:
- The current system prompt
- Any chat history from this session
- Any tool definitions
- Any session state or memory

Returns: The raw text response from the target model. No tool calls, no streaming —
just the model's direct text completion for the prompt you provide.

Constraints:
- The target model must have tool_call_available=YES. Use llm_list() to check.
- Subject to rate limiting (default: 3 calls per 20 seconds per session).
  Exceeding the limit auto-disables all llm_call models for this session.
- Each model has a timeout (default: 60s). Slow or local models may need longer.
- Returns a descriptive error string if the model is unavailable, rate-limited, or times out.

When to use:
- Call llm_list() first to discover available models and their capabilities.
- Use any model with tool_call_available=YES. Do not assume specific model names —
  available models vary by deployment.

Workflow:
1. Call llm_list() to identify a model with tool_call_available=YES.
2. Call llm_call_clean(model="<name>", prompt="<your complete self-contained prompt>").
3. Use the returned text as needed in your response.

Data handoff pattern (IMPORTANT):
The target model has NO tools. It cannot call url_extract, db_query, or any other tool.
YOU must gather data first using your own tools, then embed the results in the prompt.

Example — extracting a page and having another model summarize it:
  Step 1: url_extract(method="tavily", url="https://example.com", query="key topic")
  Step 2: llm_call_clean(model="Win11Local", prompt="Summarize this article:\n\n<paste extracted content here>")

Example — querying the DB and having another model analyze it:
  Step 1: db_query(sql="SELECT * FROM sales WHERE date > '2026-01-01'")
  Step 2: llm_call_clean(model="Win11Local", prompt="Analyze this sales data and identify trends:\n\n<paste query results here>")

The prompt you craft for llm_call_clean is your only communication channel with the target model.
Include all context the target model needs — data, instructions, and desired output format — in that prompt.
